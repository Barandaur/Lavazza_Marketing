{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most important functions:\n",
    "\n",
    "- Download ids (or ids + info) of a Twitter page followers\n",
    "- Download the pages liked by a certain user\n",
    "- Eliminate all duplicates ids of users who follow a certain page\n",
    "\n",
    "\n",
    "Most functions can be called at the same time, as they don't overlap; for example, one can create a copy of the notebook and download ids in one and pages liked in the other. While doing so, though, it is ideal to start the call in the second notebook as the first one is sleeping, or to use different databases, in order to avoid occasional locked database errors (OperationalError: database is locked).\n",
    "\n",
    "\n",
    "\n",
    "We built 2 different versions of both get_followers and get_followers_ids, solving different needs. The smartest implementation overall is the *faster version of get_followers*, where we check the tweepy_page in order to stop downloading followers when we reach the last page (very efficient and effective). It is a thought we had only after a while, so other functions could gain in efficiency by being adapted to use this same method\n",
    "\n",
    "\n",
    "* [Tweepy rate limits](https://developer.twitter.com/en/docs/basics/rate-limits)\n",
    "* [Tweepy count limits](https://developer.twitter.com/en/docs/accounts-and-users/follow-search-get-users/api-reference/get-followers-list)\n",
    "* [Tweepy documentation](http://docs.tweepy.org/en/latest/)\n",
    "\n",
    "\n",
    "# Index:\n",
    "    \n",
    "- [Libraries, Database and general functions](#Funzioni) (run the first two cells)\n",
    "- [Get Followers](#get_followers);   &nbsp;&nbsp;&nbsp;&nbsp;[faster version](#faster_get_followers)\n",
    "- [Get Followers ids](#get_followers_ids);     &nbsp;&nbsp;&nbsp;&nbsp;[faster version](#faster)\n",
    "- [Get Pages liked by an user](#get_pages)\n",
    "- [Dataset cleaning and viewing](#dataset)\n",
    "\n",
    "\n",
    "## DB Tables\n",
    "\n",
    "\n",
    "description of the tables in our database (note: we left only information related to 'lavazzagroup' to subset it):\n",
    "- _Users_  \n",
    ">  contains users who follow a page along with other info on them; \n",
    ">\n",
    ">\n",
    "> columns: ['twitter_page', 'user_id', 'description', 'location', 'status_count', 'follow_count', 'friend_count', 'changed_img', 'changed_theme', 'pages_liked']\n",
    "\n",
    "- _Cursor2_ \n",
    ">  contains the tweepy page at which we arrived using get_followers on a twitter page;\n",
    "\n",
    "- _twitter_data_ \n",
    "> contains only the ids of people who follow a brand;\n",
    ">\n",
    ">\n",
    ">columns = ['brand', 'followerid']\n",
    "\n",
    "- _Cursor_ \n",
    ">  contains the tweepy page at which we arrived using get_followers_ids on a twitter page;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='Funzioni'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries, database, general functions\n",
    "\n",
    "### <font color = #ff9900 >Please modify the path! </font>\n",
    "\n",
    "It is necessary to run the following two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the db: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json \n",
    "\n",
    "# path\n",
    "b_dir= r'C:/Users/Eugen/Documents/Uni/1 Marketing/API stuff' \n",
    "# load the auth file with all the apps\n",
    "c_auths = json.load(open(os.path.join(b_dir)+'/chain_au.txt'))\n",
    "\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# define the function we will use to connect to the db. It can also be used to get the df or create\n",
    "# a new table with specific columns\n",
    "def connect_to_sql(database_name = 'Brand_followers.db', \n",
    "                   table_name = 'twitter_data', \n",
    "                   column_names = ['brand', 'followerid'],\n",
    "                   create_table = False,\n",
    "                   return_df = False):\n",
    "    \n",
    "    con = sqlite3.connect(database_name) #create or connect to the db\n",
    "    cur = con.cursor()\n",
    "    \n",
    "    if create_table == True:\n",
    "        # generating a string out of the column names\n",
    "        str_column_names = '('\n",
    "        for i in column_names:\n",
    "            str_column_names += str(i) + ','\n",
    "        str_column_names = str_column_names[:-1] + ')'\n",
    "        cur.execute(\"CREATE TABLE IF NOT EXISTS \"+ table_name + str_column_names)\n",
    "    \n",
    "    if return_df == True:\n",
    "        query = \"SELECT * FROM \" + table_name    \n",
    "        df = pd.read_sql_query(query, con)\n",
    "        return con, cur, df\n",
    "    else:\n",
    "        return con, cur\n",
    "    \n",
    "    \n",
    "# define the db to connect to\n",
    "database_to_use = 'Brand_Followers.db'\n",
    "\n",
    "# connect to the db\n",
    "con, cur =     connect_to_sql(database_name = database_to_use, \n",
    "                              return_df = False)  \n",
    "\n",
    "# print table names\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(\"Tables in the db:\", cur.fetchall()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate( wait_T_or_F = True,     b_dir= r'C:/Users/Eugen/Documents/Uni/1 Marketing/API stuff' ):\n",
    "    '''   \n",
    "    Purpose: authenticate on twitter and create the object connected to tweepy\n",
    "    \n",
    "    Arguments: wait_T_or_F = whether to wait on rate limit\n",
    "    '''\n",
    "    \n",
    "    auths = json.load(open(os.path.join(b_dir)+'/au.txt'))\n",
    "    auth = tweepy.OAuthHandler(auths[0], auths[1])\n",
    "    auth.set_access_token(auths[2], auths[3]) \n",
    "    api=tweepy.API(auth, \n",
    "                   wait_on_rate_limit        = wait_T_or_F, \n",
    "                   wait_on_rate_limit_notify = wait_T_or_F)\n",
    "    \n",
    "    return api \n",
    "\n",
    "\n",
    "def c_authenticate(c_auths, app_num, wait_T_or_F = True):\n",
    "    '''\n",
    "     Purpose: same as authenticate, but allows to login in a specific app among a list of apps\n",
    "     \n",
    "     Arguments: c_auths: .txt file with a list of apps credentials (list of lists, as app\n",
    "                         credentials are stored in a list)\n",
    "                app_num: the app to use \n",
    "\n",
    "    '''   \n",
    "            \n",
    "    auth = tweepy.OAuthHandler(c_auths[app_num][0], c_auths[app_num][1])\n",
    "    auth.set_access_token(c_auths[app_num][2], c_auths[app_num][3]) \n",
    "    #print(c_auths[app_num][0])\n",
    "    api = tweepy.API(auth, wait_on_rate_limit= wait_T_or_F, wait_on_rate_limit_notify = wait_T_or_F)\n",
    "\n",
    "    return api\n",
    "    \n",
    "\n",
    "def rem_duplicates_if(df, \n",
    "                      page_of_interest,\n",
    "                      column_w_page_name = 'twitter_page',\n",
    "                      column_with_duplicates =  'user_id'):\n",
    "    '''    \n",
    "    Purpose: returns the df subset such that df[df[column_of_interest] == page_of_interest], \n",
    "    eliminating duplicates;\n",
    "    e.g. all the ids of the followers of page_of_interest = 'lavazzagroup', without duplicate ids\n",
    "    \n",
    "    Arguments:  page_of_interest: the page whose followes ids we want, without duplicates\n",
    "                table_name: the name of the sql table to use\n",
    "                column_w_page_name: column of the table in which twitter pages are stored\n",
    "                column_with_duplicates: column containing followers ids (or, in general, the values\n",
    "                which we want to be returned, avoiding duplicates)\n",
    "\n",
    "\n",
    "    '''\n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "    \n",
    "    df = df[df[column_w_page_name] == page_of_interest]  \n",
    "    df_no_duplicates = df[[(lambda l: not bool(l))(item) for item in df.duplicated(subset=column_with_duplicates, keep='first')]]\n",
    "    \n",
    "    return df_no_duplicates\n",
    "\n",
    "def rem_duplicates_dataset(df, \n",
    "                           column_w_page_name = 'brand',\n",
    "                           column_with_duplicates =  'followerid'):\n",
    "    '''\n",
    "    Purpose: removes duplicate followers for each page, returning the whole dataset.\n",
    "             It works operating page by page, or we would remove also people who follow\n",
    "             several different pages stored in our dataset\n",
    "    '''\n",
    "    if len(df) == 0:\n",
    "        print(\"invalid df\")\n",
    "        return df\n",
    "    print(\"Initial shape\", df.shape)  \n",
    "    \n",
    "    # creating array with twitter pages \n",
    "    twitter_pages = df[column_w_page_name].unique()\n",
    "\n",
    "    # use rem_duplicates_if on each twitter page, collecting the dfs and concatenating them at the end\n",
    "    temp_list = []\n",
    "    for twitter_page in twitter_pages:\n",
    "        df_clean = rem_duplicates_if(df, page_of_interest = twitter_page, column_w_page_name = column_w_page_name, column_with_duplicates=column_with_duplicates)\n",
    "        temp_list.append(df_clean)\n",
    "\n",
    "    df_final = pd.concat(temp_list)\n",
    "    print(\"Final shape\", df_final.shape)  \n",
    "    return df_final\n",
    "           \n",
    "    \n",
    "def num_unique_followers(table_name, column_w_page_name, column_with_duplicates, page_of_interest):\n",
    "    '''\n",
    "    Purpose: calculate the number of unique followers we have for a twitter page,\n",
    "             given an sql table, the columns and the twitter page \n",
    "    '''\n",
    "    query = \"SELECT * FROM {} where {} = '{}'\".format(table_name, column_w_page_name, page_of_interest)\n",
    "    df2 = pd.read_sql_query(query, con)\n",
    "    return len(df2[\"{}\".format(column_with_duplicates)].unique())               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='get_followers'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Followers\n",
    "[Back on top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_followers   (twitter_page: str ,\n",
    "                     api,\n",
    "                     column_with_duplicates =  'user_id',\n",
    "                     column_w_page_name = 'twitter_page',\n",
    "                     table_name = 'Users',\n",
    "                     *prior_info # insert num_ids_down\n",
    "                    ):\n",
    "    '''\n",
    "    Purpose:    Similar to get_followers_ids, but also gets other information in the process,\n",
    "                so it operates at a slower rate.\n",
    "                Opposed to get_followers_ids, it doesn't automatically iterate, and, by default,  \n",
    "                it stores in a different table ('Users' instead of 'twitter_data')\n",
    "                \n",
    "    Argomenti:  twitter_page = page whose users ids we want to retrieve\n",
    "                api = the object connected to tweepy\n",
    "                table_name = db table in which to store the data\n",
    "                column_with_duplicates = column on which we sanity check to be sure we aren't \n",
    "                                         downloading duplicates ('user_id')\n",
    "                column_w_page_name = column which contains the twitter pages names\n",
    "                prior_info: bool on whether we already downloaded some users of that page,\n",
    "                            and an int on how many we downloaded (provide 0 if the bool is False)\n",
    "    '''\n",
    "    \n",
    "    # tot num of follw of the page\n",
    "    follw_count = api.get_user(twitter_page).followers_count  # tot num of follw of a page\n",
    "    # max num of downloadable user objects per tweepy page\n",
    "    id_downl_at_step = 200\n",
    "    \n",
    "# access/create the db containing the cursors values, will be useful \n",
    "# to know at which tweepy page to start or resume the download\n",
    "    cur.execute(\"CREATE TABLE IF NOT EXISTS Cursor2(twitter_page, cursor_value)\")\n",
    "    \n",
    "    # if the page is new, start from scratch, else from the cur val in the db\n",
    "    try:\n",
    "        # if we haven't this ids already, it will give an error, prompting the except\n",
    "        query_2 = \"SELECT cursor_value FROM Cursor2 WHERE twitter_page = '{}'\".format(twitter_page)              \n",
    "        num_page = int(pd.read_sql_query(query_2, con).loc[0])\n",
    "        if not prior_info:\n",
    "            num_ids_downloaded = num_unique_followers(table_name, column_w_page_name, column_with_duplicates, twitter_page)\n",
    "        else:\n",
    "            num_ids_downloaded = prior_info[0]\n",
    "            \n",
    "    # if we have no ids or no cursor value, start from scratch\n",
    "    except:\n",
    "        num_page = -1\n",
    "        num_ids_downloaded = 0\n",
    "    \n",
    "    print(\"\\n\", twitter_page, \"has {} followers, we already have {} ids\".format(follw_count, num_ids_downloaded))    \n",
    "       \n",
    "    bag = []        \n",
    "    # get follower ids and other info on them\n",
    "    tweepy_cursor = tweepy.Cursor(api.followers, id= twitter_page, count = id_downl_at_step, cursor = num_page).pages(15)\n",
    "    for result_set in tweepy_cursor:\n",
    "        for user in result_set:\n",
    "            bag.append([twitter_page, user.id, user.description, user.location, user.statuses_count, user.followers_count,\n",
    "                      user.friends_count, user.default_profile_image, user.default_profile, np.nan])\n",
    "    # update cursor value\n",
    "    num_page = tweepy_cursor.next_cursor\n",
    "    \n",
    "    # store the info in the db. To be made more efficient, can put this step outside\n",
    "    # the function, storging less frequently\n",
    "    qst_marks = \"?,\"*len(bag[0])\n",
    "    cur.executemany(\"INSERT INTO {} VALUES ({})\".format(table_name, qst_marks[:-1]), bag) \n",
    "    \n",
    "    # if the page is new, create a value in the db, else update it\n",
    "    if num_ids_downloaded == 0: \n",
    "        cur.execute(\"INSERT INTO Cursor2 (twitter_page, cursor_value) VALUES (?,?)\", [twitter_page, num_page])\n",
    "    else: \n",
    "        cur.execute(\"UPDATE Cursor2 SET cursor_value = \" + str(num_page) + \" WHERE twitter_page ='{}'\".format(twitter_page))\n",
    "        \n",
    "    con.commit()\n",
    "    \n",
    "    # check if we really downloaded new data    \n",
    "    final_ids = num_unique_followers(table_name, column_w_page_name, column_with_duplicates, twitter_page)\n",
    "    print(\"df final length = {}, \\ndf initial length = {}, \\nnew ids added: {}\".format(final_ids, num_ids_downloaded, final_ids-num_ids_downloaded))\n",
    "    \n",
    "    # return the final number of unique ids we have\n",
    "    return final_ids\n",
    "\n",
    "\n",
    "def get_user(user_id, api, table_name = 'Users', store = True):   # up to 900 requests each 15 minutes\n",
    "    '''\n",
    "    WHAT DOES IT DO?\n",
    "    Collect other info on a user, useful if we only have the id. Stores it in the Users table\n",
    "    '''\n",
    "    try:\n",
    "        if type(user_id) == int:\n",
    "            user = api.get_user(user_id)\n",
    "            bag = [user.description, user.location, user.statuses_count, user.followers_count,\n",
    "                                  user.friends_count, user.default_profile_image, user.default_profile, user_id]\n",
    "\n",
    "\n",
    "        else: \n",
    "            bag = []\n",
    "            for single_user_id in user_id:\n",
    "                user = api.get_user(single_user_id)\n",
    "                bag.append((user.description, user.location, user.statuses_count, user.followers_count,\n",
    "                          user.friends_count, user.default_profile_image, user.default_profile, single_user_id))\n",
    "                \n",
    "        if store == True:\n",
    "            cur.executemany(\"UPDATE \"+table_name+\" SET description =?, location =?, status_count =?, follow_count =?, friend_count =?, changed_img =?, changed_theme =? WHERE user_id = ?\",  bag)\n",
    "            con.commit()\n",
    "                \n",
    "        return bag\n",
    "        \n",
    "    except:\n",
    "        print(\"Twitter error for this user\")\n",
    "        return np.nan\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_followers_chained_all_stats(twitter_page,\n",
    "                                    follw_count,\n",
    "                                    table_name = 'Users',\n",
    "                                    column_w_page_name = 'twitter_page',\n",
    "                                    column_with_duplicates = 'user_id',\n",
    "                                    b_dir= r'C:/Users/Eugen/Documents/Uni/1 Marketing/API stuff'):\n",
    "    '''\n",
    "    Purpose: unless we generate a number of apps, we will have sleeping times.\n",
    "             To make the most of them, chain get_followers with get_followers_ids\n",
    "             and user methods to be able to download more ids and info on users\n",
    "             every 15 minutes. These calls, despite being of the same type (GET),\n",
    "             don't interfere with each other's rate limits\n",
    "    '''\n",
    "\n",
    "    # load the file with the auths\n",
    "    c_auths = json.load(open(os.path.join(b_dir)+'/chain_au.txt'))\n",
    "    \n",
    "    num_ids_downloaded = num_unique_followers(table_name, column_w_page_name, column_with_duplicates, twitter_page)\n",
    "    \n",
    "    print(\"\\n\", twitter_page, \"has {} followers, we already have {} ids\".format(follw_count, num_ids_downloaded))\n",
    "    \n",
    "    # at each iter we get 3200 ids and info, per each app used\n",
    "    num_iterations = int(np.ceil( (follw_count - num_ids_downloaded) / (3200* len(c_auths)) ))\n",
    "    time_check = False\n",
    "    \n",
    "    # at each iteration we go through all apps and call different methods\n",
    "    for iteration in range(0, num_iterations):\n",
    "        print(\"We are at iteration {} on {}\".format(iteration + 1, num_iterations))\n",
    "        \n",
    "        # except the first time, sleep if needed\n",
    "        if time_check == True:     \n",
    "            time_elapsed = time.time() - start_time \n",
    "            if time_elapsed < (15*60) + 5:\n",
    "                print(\"So far {} seconds have passed, need to sleep for {}\".format(time_elapsed, 60*15 - time_elapsed))\n",
    "                time.sleep(time_elapsed)\n",
    "\n",
    "        # switching auths\n",
    "        for app_num in range(len(c_auths)):\n",
    "            print(\"Using App number: \", app_num)\n",
    "            api = c_authenticate(c_auths, app_num)\n",
    "\n",
    "           # starting to retrieve data, inizializing sleep stuff\n",
    "            print(\"\\nGETTING FOLLOWERS\")  \n",
    "            if app_num == 0:\n",
    "                start_time = time.time()\n",
    "\n",
    "            num_ids_downloaded = get_followers(twitter_page, # gets 3000 ids and stats\n",
    "                                               api,\n",
    "                                               column_with_duplicates,\n",
    "                                               column_w_page_name,\n",
    "                                               table_name,\n",
    "                                               num_ids_downloaded) \n",
    "            time_check = True\n",
    "            \n",
    "            if num_ids_downloaded >= follw_count*0.97:\n",
    "                print(\"\\n We already have all ids; terminating the call\")\n",
    "                print(\"ids we have:\", str(num_ids_downloaded), \"ids of the page: \", str(follw_count) )\n",
    "                return #breaks all loops\n",
    "\n",
    "            print(\"\\nGETTING FOLLOWERS IDS\")\n",
    "            \n",
    "            # luckily, we can use the same cursor we created above\n",
    "            try:\n",
    "                query_2 = \"SELECT cursor_value FROM Cursor2 WHERE twitter_page = '{}'\".format(twitter_page)  \n",
    "                num_page = int(pd.read_sql_query(query_2, con).loc[0])\n",
    "                \n",
    "            # added an except in case the above part got skipped for any reason, and\n",
    "            # we have no ids (thus no cursor value) for this page\n",
    "            except:\n",
    "                num_page= -1\n",
    "                \n",
    "            user_ids = []\n",
    "            # based on the speed of computer and connection, we can get multiples of \n",
    "            # 200 ids at this step. Chaining through 5 apps, it leads to multiples of 1000\n",
    "            # more ids and info every 15 mins (up to a maxiumum of 8000, due to get_user rate\n",
    "            # limit)\n",
    "            tweepy_cursor = tweepy.Cursor(api.followers_ids, id = twitter_page, cursor = num_page, count = 200 ).pages(1) \n",
    "            for id_list in tweepy_cursor:\n",
    "                user_ids.extend(id_list)\n",
    "            num_page = tweepy_cursor.next_cursor\n",
    "                        \n",
    "            g = list(zip([twitter_page]*len(user_ids), user_ids))\n",
    "\n",
    "            cur.executemany(\"INSERT INTO {}(twitter_page, user_id) VALUES (?,?)\".format(table_name), (g)) \n",
    "            cur.execute(\"UPDATE Cursor2 SET cursor_value = \" + str(num_page) + \" WHERE twitter_page ='{}'\".format(twitter_page))\n",
    "            con.commit()\n",
    "            \n",
    "            print(\"\\nGETTING STATS\")\n",
    "            '''\n",
    "            was used initially to get also some liked pages, but not worth it much\n",
    "            \n",
    "            print(\"\\nGETTING STATS AND PAGES LIKED BY IDS\")\n",
    "            #i = 0\n",
    "            id_page_list = []\n",
    "            user_info = []\n",
    "            for user_id in user_ids: \n",
    "                \n",
    "                #get some of the pages liked by the first 15 users in the list\n",
    "                #if i < 15:\n",
    "                #    i += 1\n",
    "                #    pages = get_pages(_id = user_id, api = api)\n",
    "                #    id_page_list.append((user_id, pages))\n",
    "                #    if i == 14:\n",
    "                #        store_pages([], 2593342898, 'Users', 'pages_liked', # initialised just to feed the keyword argument\n",
    "                #                    id_page_list) # the last argument is the important one\n",
    "\n",
    "                info = get_user(user_id, api = api) # can get up to 800 user info\n",
    "                user_info.append(info)\n",
    "            # store user info\n",
    "            cur.execute(\"UPDATE \"+table_name+\" SET description =?, location =?, status_count =?, follow_count =?, friend_count =?, changed_img =?, changed_theme =? WHERE user_id = ?\",  user_info)\n",
    "            '''\n",
    "            get_user(user_ids, api = api, store = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' GET ALL FOLLOWERS OF A TWITTER PAGE, TOGETHER WITH INFO ON THEM '''\n",
    "# slower than getting only ids, as it also has more stringent rate limits\n",
    "\n",
    "twitter_page = 'Starbucks'\n",
    "\n",
    "api = c_authenticate(c_auths, 0)\n",
    "follw_count = api.get_user(twitter_page).followers_count \n",
    "\n",
    "get_followers_chained_all_stats(twitter_page, follw_count = follw_count , b_dir = b_dir) \n",
    "\n",
    "''' \n",
    "the above function is worth if one is working with few apps,\n",
    "else it's better to just use get_followers or the function below'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='faster_get_followers'> </a>\n",
    "### Faster and smarter version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using app num 0\n",
      "1641896534143447276\n",
      "\n",
      "new ids added: 3000\n",
      "using app num 1\n",
      "1609035824861973338\n",
      "\n",
      "new ids added: 3000\n",
      "using app num 2\n",
      "1592754784274484941\n",
      "\n",
      "new ids added: 3000\n",
      "using app num 3\n",
      "1579357836446908042\n",
      "\n",
      "new ids added: 3000\n",
      "using app num 4\n",
      "1569673015652300793\n",
      "\n",
      "new ids added: 3000\n",
      "using app num 5\n",
      "0\n",
      "\n",
      " Done\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "                FASTER VERSION\n",
    "                \n",
    "here we experimented a while loop.\n",
    "We store only\n",
    "before (eventually) sleeping, and at the end\n",
    "'''\n",
    "\n",
    "# modify this\n",
    "twitter_page = 'bncalcio'\n",
    "\n",
    "def store(twitter_page, num_page, bag):\n",
    "\n",
    "def store(twitter_page, num_page, bag):\n",
    "    \n",
    "    if bag != []:      \n",
    "        # if the page is new, create a value for the cursors in the db, else update it\n",
    "        try:\n",
    "            cur.execute(\"UPDATE Cursor2 SET cursor_value = \" + str(num_page) + \" WHERE twitter_page ='{}'\".format(twitter_page))\n",
    "        except: \n",
    "            cur.execute(\"INSERT INTO Cursor2 (twitter_page, cursor_value) VALUES (?,?)\", [twitter_page, num_page])\n",
    "\n",
    "        \n",
    "        qst_marks = \"?,\"*len(bag[0])\n",
    "        cur.executemany(\"INSERT INTO {} VALUES ({})\".format(table_name, qst_marks[:-1]), bag) \n",
    "        bag = [] \n",
    "    con.commit()\n",
    "    \n",
    "    return bag\n",
    "\n",
    "table_name = 'Users'\n",
    "column_w_page_name = 'twitter_page'\n",
    "column_with_duplicates = 'user_id'\n",
    "\n",
    "cur.execute(\"CREATE TABLE IF NOT EXISTS Cursor2(twitter_page, cursor_value)\")\n",
    "# if the page is new, start from scratch, else from the cur val in the db\n",
    "try:\n",
    "    # will prompt the except if the page is new\n",
    "    query_2 = \"SELECT cursor_value FROM Cursor2 WHERE twitter_page = '{}'\".format(twitter_page)              \n",
    "    num_page = int(pd.read_sql_query(query_2, con).loc[0])\n",
    "        \n",
    "# if we have no ids or no cursor value, start from scratch\n",
    "except:\n",
    "    print(\"Starting from the first page\")\n",
    "    num_page = 0\n",
    "\n",
    "app_num = len(c_auths) -1\n",
    "bag = []\n",
    "start_time = 2588440044 #to avoid sleeping at first iter\n",
    "#s = set() # useful for debugging\n",
    "\n",
    "# we will stop the call when we will reach last tweepy page\n",
    "while True:\n",
    "    \n",
    "    # login to an app\n",
    "    api = c_authenticate(c_auths, app_num)\n",
    "    # useful for debugging\n",
    "    #initial_s_len = len(s)\n",
    "\n",
    "# change app and reset app if we passed through all,\n",
    "# sleep if needed, store into the db the cursor\n",
    "# and the pages retrieved\n",
    "    app_num += 1\n",
    "    if app_num == len(c_auths):\n",
    "        app_num = 0\n",
    "        bag = store(twitter_page, num_page, bag)\n",
    "        \n",
    "        time.sleep(max(0, 15*60 -  time.time() - start_time ))\n",
    "        start_time = time.time()\n",
    "    \n",
    "# get follower ids and other info on them\n",
    "    print(\"using app num\", app_num)\n",
    "    tweepy_cursor = tweepy.Cursor(api.followers, id= twitter_page, count = 200, cursor = num_page).pages(15)\n",
    "    for result_set in tweepy_cursor:\n",
    "        for user in result_set:\n",
    "            bag.append([twitter_page, user.id, user.description, user.location, user.statuses_count, user.followers_count,\n",
    "                      user.friends_count, user.default_profile_image, user.default_profile, np.nan])\n",
    "            #s.add(user.id)    \n",
    "\n",
    "    # update cursor value\n",
    "    num_page = tweepy_cursor.next_cursor\n",
    "    print(\"Cursor value = \", num_page)\n",
    "    \n",
    "    # when we reach last tweepy page,\n",
    "    # it starts back from 0. Thus, if\n",
    "    # we reach page 0, break the loop\n",
    "    if num_page == 0 :\n",
    "        break\n",
    "\n",
    "# check if we really downloaded new data    \n",
    "    #new_ids_downl = len(s) - initial_s_len\n",
    "    #print(\"\\nnew ids added: {}\".format(new_ids_downl))\n",
    "\n",
    "# we store the cursor coming before page 0.\n",
    "# This way, if we call this function on a page whose\n",
    "# followers we already have, it goes only for 1 iter\n",
    "# before stopping (of course one could add a check to\n",
    "# avoid wasting even that single app, but we found this\n",
    "# to a more elegant solution)\n",
    "store(twitter_page, tweepy_cursor.prev_cursor, bag)\n",
    "print(\"\\n Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitter_page</th>\n",
       "      <th>user_id</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>status_count</th>\n",
       "      <th>follow_count</th>\n",
       "      <th>friend_count</th>\n",
       "      <th>changed_img</th>\n",
       "      <th>changed_theme</th>\n",
       "      <th>pages_liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>bncalcio</td>\n",
       "      <td>2185564980</td>\n",
       "      <td>Sabes que no dije mentiras 😉\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>Funafuti, Tuvalu 🇹🇻</td>\n",
       "      <td>9150.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>1660.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>bncalcio</td>\n",
       "      <td>1129430611058450434</td>\n",
       "      <td>Suivez l'actualité du football en temps réel s...</td>\n",
       "      <td></td>\n",
       "      <td>519.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bncalcio</td>\n",
       "      <td>725062262822723585</td>\n",
       "      <td>footballer</td>\n",
       "      <td></td>\n",
       "      <td>60.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>2301.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>bncalcio</td>\n",
       "      <td>1063870559522615296</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>25.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>bncalcio</td>\n",
       "      <td>1014881241550778369</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2696.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>793.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16492</td>\n",
       "      <td>bncalcio</td>\n",
       "      <td>815182819676880896</td>\n",
       "      <td>Giornalista pubblicista iscritto all'Ordine de...</td>\n",
       "      <td>Pisa, Toscana</td>\n",
       "      <td>1576.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16493</td>\n",
       "      <td>bncalcio</td>\n",
       "      <td>2373917562</td>\n",
       "      <td>OttoChannel è visibile in tutta la Campania su...</td>\n",
       "      <td>Campania, Italia</td>\n",
       "      <td>20524.0</td>\n",
       "      <td>834.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16494</td>\n",
       "      <td>bncalcio</td>\n",
       "      <td>111658199</td>\n",
       "      <td>Antipatica e antifascista</td>\n",
       "      <td>Benevento, Campania</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>552.0</td>\n",
       "      <td>1244.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16495</td>\n",
       "      <td>bncalcio</td>\n",
       "      <td>1511807881</td>\n",
       "      <td>#WebManager, #WebDeveloper and much more... On...</td>\n",
       "      <td>Italia</td>\n",
       "      <td>160.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16496</td>\n",
       "      <td>bncalcio</td>\n",
       "      <td>87243594</td>\n",
       "      <td>Il twitter feed ufficiale delle notizie di Ott...</td>\n",
       "      <td>Campania, Italia</td>\n",
       "      <td>225630.0</td>\n",
       "      <td>5960.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16497 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      twitter_page              user_id  \\\n",
       "0         bncalcio           2185564980   \n",
       "1         bncalcio  1129430611058450434   \n",
       "2         bncalcio   725062262822723585   \n",
       "3         bncalcio  1063870559522615296   \n",
       "4         bncalcio  1014881241550778369   \n",
       "...            ...                  ...   \n",
       "16492     bncalcio   815182819676880896   \n",
       "16493     bncalcio           2373917562   \n",
       "16494     bncalcio            111658199   \n",
       "16495     bncalcio           1511807881   \n",
       "16496     bncalcio             87243594   \n",
       "\n",
       "                                             description             location  \\\n",
       "0      Sabes que no dije mentiras 😉\\n\\n\\n\\n\\n\\n\\n\\n\\n...  Funafuti, Tuvalu 🇹🇻   \n",
       "1      Suivez l'actualité du football en temps réel s...                        \n",
       "2                                             footballer                        \n",
       "3                                                                               \n",
       "4                                                                               \n",
       "...                                                  ...                  ...   \n",
       "16492  Giornalista pubblicista iscritto all'Ordine de...        Pisa, Toscana   \n",
       "16493  OttoChannel è visibile in tutta la Campania su...     Campania, Italia   \n",
       "16494                          Antipatica e antifascista  Benevento, Campania   \n",
       "16495  #WebManager, #WebDeveloper and much more... On...               Italia   \n",
       "16496  Il twitter feed ufficiale delle notizie di Ott...     Campania, Italia   \n",
       "\n",
       "       status_count  follow_count  friend_count  changed_img  changed_theme  \\\n",
       "0            9150.0         227.0        1660.0          0.0            1.0   \n",
       "1             519.0          22.0         305.0          0.0            1.0   \n",
       "2              60.0         123.0        2301.0          0.0            1.0   \n",
       "3              25.0           4.0         159.0          0.0            1.0   \n",
       "4            2696.0          27.0         793.0          0.0            1.0   \n",
       "...             ...           ...           ...          ...            ...   \n",
       "16492        1576.0          82.0         544.0          0.0            0.0   \n",
       "16493       20524.0         834.0          25.0          0.0            0.0   \n",
       "16494        1126.0         552.0        1244.0          0.0            0.0   \n",
       "16495         160.0         259.0         131.0          0.0            1.0   \n",
       "16496      225630.0        5960.0          22.0          0.0            0.0   \n",
       "\n",
       "      pages_liked  \n",
       "0            None  \n",
       "1            None  \n",
       "2            None  \n",
       "3            None  \n",
       "4            None  \n",
       "...           ...  \n",
       "16492        None  \n",
       "16493        None  \n",
       "16494        None  \n",
       "16495        None  \n",
       "16496        None  \n",
       "\n",
       "[16497 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check that it worked\n",
    "df = pd.read_sql_query(\"select * from Users where twitter_page ='{}'\".format(twitter_page), con)\n",
    "\n",
    "rem_duplicates_if(df, page_of_interest = twitter_page, \n",
    "                     column_w_page_name = column_w_page_name,\n",
    "                     column_with_duplicates =  column_with_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='get_followers_ids'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Followers Ids\n",
    "\n",
    "[Back on top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it was born as a standalone, single-user function, but it is pretty flexible, so, thanks\n",
    "# to max_iters, it can be used aswell in a loop, while changing the app externally.\n",
    "# One thing to note is that by storing in sql the tweepy page we reached, we can resume\n",
    "# downloads without losing progress, if working across several days or in case of errors.\n",
    "# This function aims at easiness of use and nice prints, we will show a more optimized function\n",
    "# later\n",
    "def get_followers_ids(twitter_page: str ,\n",
    "                     column_with_duplicates =  'followerid',\n",
    "                     column_w_page_name = 'brand',\n",
    "                     table_name = 'twitter_data',\n",
    "                     id_downl_at_step = 5000,\n",
    "                    *max_iters \n",
    "                    ):\n",
    "    '''\n",
    "    Purpose:    Downloads ids of followers of a page and stores them in the db and table in use,\n",
    "                inserting the page name in column_w_page_name, and ids in the column follower_id\n",
    "                   \n",
    "    Arguments:  twitter_page = page whose users ids we want to retrieve\n",
    "                table_name = db table in which to store the data\n",
    "                column_with_duplicates = column on which we sanity check to be sure we aren't \n",
    "                                         downloading duplicates ('followerid')\n",
    "                column_w_page_name = column which contains the twitter pages names\n",
    "                max_iters = if we want to limit the number of iterations\n",
    "    '''\n",
    "    # calculate how many ids we already have\n",
    "    num_ids_downloaded = num_unique_followers(table_name, column_w_page_name, column_with_duplicates, twitter_page)\n",
    "    # tot num of follw of the twitter page\n",
    "    follw_count = api.get_user(twitter_page).followers_count   \n",
    "    print(\"\\n\", twitter_page, \"has {} followers, we already have {} ids\".format(follw_count, num_ids_downloaded))\n",
    "    \n",
    "    # access/create the db containing the cursors values, will be useful \n",
    "    # to know at which tweepy page to start (or resume) the download\n",
    "    cur.execute(\"CREATE TABLE IF NOT EXISTS Cursor(twitter_page, cursor_value)\")  \n",
    "    # if we already have some ids, start from the cur val in the db\n",
    "    try:\n",
    "        # if the page is new, it will give an error, prompting the except\n",
    "        query_2 = \"SELECT cursor_value FROM Cursor WHERE twitter_page = '{}'\".format(twitter_page)              \n",
    "        num_page = int(pd.read_sql_query(query_2, con).loc[0])\n",
    "    # else start from scratch\n",
    "    except:\n",
    "        num_page = -1\n",
    "        \n",
    "        # if we already have all ids, just return\n",
    "        # this will avoid being struck in a loop when we can't download\n",
    "        # all users, for example because, since the first call,\n",
    "        # new users started or stopped following the page\n",
    "        if num_ids_downloaded > (follw_count*0.97):\n",
    "            print(\"\\nWe already have almost all ids ({} on {}). Interrupting the call\".format(num_ids_downloaded,follw_count))\n",
    "            return\n",
    "        \n",
    "        # a bit brutal, but, if we downloaded some ids without this function,\n",
    "        # we might not have a cursor value and we wouldn't know from where to resume \n",
    "        # the download, meaning we must start from scratch, so let's clean at least.\n",
    "        # Hopefully we always use this funct and don't incur in this scenario.\n",
    "        # Avoid this scenario if we already have almost all ids\n",
    "        elif num_ids_downloaded != 0:\n",
    "            cur.execute(\"DELETE FROM {} where {} == '{}'\".format(table_name, column_w_page_name, twitter_page))\n",
    "            con.commit()\n",
    "            num_ids_downloaded = 0\n",
    "            print(\"Didn't know from where to resume download. Starting from scratch\")\n",
    "\n",
    "            \n",
    "    # calculate num of iterations needed \n",
    "    if max_iters: #if we set a limit to the iterations\n",
    "        # min to check if we need even less than the provided num of iters\n",
    "        num_iterations = min(max_iters[0][0], int(np.ceil( (follw_count - num_ids_downloaded) / (id_downl_at_step * 15) )))\n",
    "    else:\n",
    "        # at each iter we get 15*id_downl_at_step ids, we want to do as many iters as\n",
    "        # necessary to retrieve all ids we still have to retrieve\n",
    "        num_iterations = int(np.ceil( (follw_count - num_ids_downloaded) / (id_downl_at_step * 15) ))\n",
    "    print(\"I will go for {} iterations \\n\".format(num_iterations))   \n",
    "    \n",
    "    for iteration in range(0, num_iterations):\n",
    "        \n",
    "        print(\"We are at iter {}, on {}\".format(iteration+1,num_iterations))\n",
    "     \n",
    "        # just to print more accurate informations\n",
    "        start_page = (int(np.floor(num_ids_downloaded / id_downl_at_step)) - 1)\n",
    "        # at each iteration we want to go through all tweepy pages \n",
    "        # necessary to get all ids (max 15 to avoid rate limits)\n",
    "        num_of_pages_to_scroll =  min( 15, int(np.ceil( (follw_count - num_ids_downloaded) / (id_downl_at_step) )))\n",
    "\n",
    "        for page in range(start_page, start_page + num_of_pages_to_scroll):\n",
    "                        \n",
    "            # 'page' and 'num_page' contain the same info, but 'num_pages' is in the\n",
    "            # 'language' used by tweepy, that is, page 2 may be expressed as page 283942837\n",
    "            # we will print user level readable information using 'page', while we will feed\n",
    "            # 'num_pages' to tweepy\n",
    "            print(\"I'm at page \", page)\n",
    "            tweepy_cursor = tweepy.Cursor(api.followers_ids, id = twitter_page, cursor = num_page).pages(1) \n",
    "            lista_temp = []\n",
    "            lista_temp.extend(tweepy_cursor)  \n",
    "            \n",
    "            # g contains the page name and the follower id, for each of the ids retrieved\n",
    "            g = list(zip([twitter_page]*len(lista_temp[0]), lista_temp[0])) \n",
    "# can be made more efficient by storing outside the loop, or even the function,\n",
    "# but, unless one generates a big number of apps, there are sleeping times anyways, \n",
    "# so it's worth to store each time we can\n",
    "            cur.executemany(\"INSERT INTO {} VALUES (?,?)\".format(table_name), (g)) \n",
    "            num_page = tweepy_cursor.next_cursor\n",
    "            \n",
    "        # check how many unique ids we have now, to see if things worked out well \n",
    "        new_num_ids = num_unique_followers(table_name, column_w_page_name, column_with_duplicates, twitter_page)\n",
    "        \n",
    "        # if the page is new, create a value for the tweepy_page in the db\n",
    "        if num_ids_downloaded == 0 and iteration < 1: \n",
    "            cur.execute(\"INSERT INTO Cursor (twitter_page, cursor_value) VALUES (?,?)\", [twitter_page, num_page])\n",
    "        #  else we already have it, so just update it\n",
    "        else: \n",
    "            cur.execute(\"UPDATE Cursor SET cursor_value = \" + str(num_page) + \" WHERE twitter_page ='{}'\".format(twitter_page))\n",
    "\n",
    "        con.commit()\n",
    "        \n",
    "        print('finished going through pages for this iteration\\n')\n",
    "        print(\"df final length = {}, \\ndf initial length = {}, \\nnew ids added: {}\".format(new_num_ids, num_ids_downloaded, new_num_ids-num_ids_downloaded))\n",
    "        print(\"\\nnum pages visited at this iteration: {}\\n\".format(num_of_pages_to_scroll))\n",
    "        \n",
    "        if new_num_ids >= follw_count:\n",
    "            print(\"Done. We have collected {} ids, {} currently has {} ids\".format(new_num_ids, twitter_page, follw_count))\n",
    "            print('--- breaking loop ---')\n",
    "            break\n",
    "        else:\n",
    "            print('still going strong')\n",
    "        \n",
    "        # sleep 15 min except at the last iter. It won't sleep if we set max iters to 1, so \n",
    "        # we can call this function with max_iters = 1 while externally taking care of sleep\n",
    "        # and app switch\n",
    "        if iteration != num_iterations -1:          \n",
    "            #get_followers(twitter_page = twitter_page)\n",
    "            time.sleep(60*8)\n",
    "            print(\"halfway back to work zzz \\n\")\n",
    "            time.sleep(60*7)\n",
    "\n",
    "# Return True if we already have all followers and we skipped the iterations. \n",
    "# We will use this bool when calling this function on a set of pages, to switch to the\n",
    "# next page to download, in case we already have all ids for the current page and\n",
    "# we bypassed the 'break' in the above iteration loop\n",
    "    if num_iterations == 0:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading followers of OliviaPalermo\n",
      "using app num  0\n",
      "\n",
      " OliviaPalermo has 418414 followers, we already have 75001 ids\n",
      "I will go for 1 iterations \n",
      "\n",
      "We are at iter 1, on 1\n",
      "I'm at page  14\n",
      "I'm at page  15\n",
      "I'm at page  16\n",
      "I'm at page  17\n",
      "I'm at page  18\n",
      "I'm at page  19\n",
      "I'm at page  20\n",
      "I'm at page  21\n",
      "I'm at page  22\n",
      "I'm at page  23\n",
      "I'm at page  24\n",
      "I'm at page  25\n",
      "I'm at page  26\n",
      "I'm at page  27\n",
      "I'm at page  28\n",
      "finished going through pages for this iteration\n",
      "\n",
      "df final length = 150001, \n",
      "df initial length = 75001, \n",
      "new ids added: 75000\n",
      "\n",
      "num pages visited at this iteration: 15\n",
      "\n",
      "still going strong\n",
      "using app num  1\n",
      "\n",
      " OliviaPalermo has 418414 followers, we already have 150001 ids\n",
      "I will go for 1 iterations \n",
      "\n",
      "We are at iter 1, on 1\n",
      "I'm at page  29\n",
      "I'm at page  30\n",
      "I'm at page  31\n",
      "I'm at page  32\n",
      "I'm at page  33\n",
      "I'm at page  34\n",
      "I'm at page  35\n",
      "I'm at page  36\n",
      "I'm at page  37\n",
      "I'm at page  38\n",
      "I'm at page  39\n",
      "I'm at page  40\n",
      "I'm at page  41\n",
      "I'm at page  42\n",
      "I'm at page  43\n",
      "finished going through pages for this iteration\n",
      "\n",
      "df final length = 225001, \n",
      "df initial length = 150001, \n",
      "new ids added: 75000\n",
      "\n",
      "num pages visited at this iteration: 15\n",
      "\n",
      "still going strong\n",
      "using app num  2\n",
      "\n",
      " OliviaPalermo has 418415 followers, we already have 225001 ids\n",
      "I will go for 1 iterations \n",
      "\n",
      "We are at iter 1, on 1\n",
      "I'm at page  44\n",
      "I'm at page  45\n",
      "I'm at page  46\n",
      "I'm at page  47\n",
      "I'm at page  48\n",
      "I'm at page  49\n",
      "I'm at page  50\n",
      "I'm at page  51\n",
      "I'm at page  52\n",
      "I'm at page  53\n",
      "I'm at page  54\n",
      "I'm at page  55\n",
      "I'm at page  56\n",
      "I'm at page  57\n",
      "I'm at page  58\n",
      "finished going through pages for this iteration\n",
      "\n",
      "df final length = 300001, \n",
      "df initial length = 225001, \n",
      "new ids added: 75000\n",
      "\n",
      "num pages visited at this iteration: 15\n",
      "\n",
      "still going strong\n",
      "using app num  3\n",
      "\n",
      " OliviaPalermo has 418414 followers, we already have 300001 ids\n",
      "I will go for 1 iterations \n",
      "\n",
      "We are at iter 1, on 1\n",
      "I'm at page  59\n",
      "I'm at page  60\n",
      "I'm at page  61\n",
      "I'm at page  62\n",
      "I'm at page  63\n",
      "I'm at page  64\n",
      "I'm at page  65\n",
      "I'm at page  66\n",
      "I'm at page  67\n",
      "I'm at page  68\n",
      "I'm at page  69\n",
      "I'm at page  70\n",
      "I'm at page  71\n",
      "I'm at page  72\n",
      "I'm at page  73\n",
      "finished going through pages for this iteration\n",
      "\n",
      "df final length = 375001, \n",
      "df initial length = 300001, \n",
      "new ids added: 75000\n",
      "\n",
      "num pages visited at this iteration: 15\n",
      "\n",
      "still going strong\n",
      "using app num  4\n",
      "\n",
      " OliviaPalermo has 418414 followers, we already have 375001 ids\n",
      "I will go for 1 iterations \n",
      "\n",
      "We are at iter 1, on 1\n",
      "I'm at page  74\n",
      "I'm at page  75\n",
      "I'm at page  76\n",
      "I'm at page  77\n",
      "I'm at page  78\n",
      "I'm at page  79\n",
      "I'm at page  80\n",
      "I'm at page  81\n",
      "I'm at page  82\n",
      "finished going through pages for this iteration\n",
      "\n",
      "df final length = 418414, \n",
      "df initial length = 375001, \n",
      "new ids added: 43413\n",
      "\n",
      "num pages visited at this iteration: 9\n",
      "\n",
      "Done. We have collected 418414 ids, OliviaPalermo currently has 418414 ids\n",
      "--- breaking loop ---\n",
      "using app num  5\n",
      "\n",
      " OliviaPalermo has 418414 followers, we already have 418414 ids\n",
      "I will go for 0 iterations \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' SWITCHING TROUGH APPS, DOWNLOAD IDS OF FOLLOWERS OF ONE OR MORE PAGES '''\n",
    "\n",
    "# need to change only twitter_pages of interest\n",
    "column_with_duplicates =  'followerid'\n",
    "column_w_page_name = 'brand'\n",
    "table_name = 'twitter_data'\n",
    "\n",
    "twitter_pages =  [\"HandpressoNews\", \"MaisonValentino\", \"Prada\", \"Fendi\", \"PaneraiOfficial\", \"Azimut_Yachts\",\"armani\", \"Ferrari\",\"TiffanyAndCo\",\"ROLEX\",\"Bulgariofficial\", \"Lamborghini\", \"RivaYacht\", \"FerrettiGroup\"] \n",
    "app_num = 0\n",
    "executed_iterations = 0\n",
    "\n",
    "for twitter_page in twitter_pages:\n",
    "    print(\"downloading followers of\", twitter_page)\n",
    "\n",
    "    # login to the app and get the number of followers of the page\n",
    "    api = c_authenticate(c_auths, app_num = app_num)\n",
    "    follw_count = api.get_user(twitter_page).followers_count\n",
    "\n",
    "    num_iterations = int( np.ceil(follw_count/75000) ) \n",
    "    \n",
    "    for iteration in range(0, num_iterations):\n",
    "\n",
    "        if app_num % len(c_auths) == 0:        \n",
    "            start_time = time.time()\n",
    "             \n",
    "        print(\"using app num \", app_num)\n",
    "        # call to the function we created\n",
    "        bool_finished = get_followers_ids(twitter_page,\n",
    "                                         column_with_duplicates,\n",
    "                                         column_w_page_name,\n",
    "                                         table_name,\n",
    "                                         5000,\n",
    "                                         (1,) )\n",
    "        \n",
    "        # break the inner loop if we already have all followers for this page\n",
    "        if bool_finished == True:\n",
    "            break\n",
    "        \n",
    "        # each iter calls 15 times the tweepy_function, so at each iter we change app\n",
    "        app_num += 1   \n",
    "\n",
    "# every time we passed through all the apps, we make a time check and reset app_num\n",
    "        if app_num % len(c_auths) == 0:\n",
    "            sleep_time = 60*15 - (time.time() - start_time) \n",
    "            print(\"\\n sleeping for {} \\n\".format(sleep_time))\n",
    "            time.sleep(max(0, sleep_time))\n",
    "            app_num = 0\n",
    "        \n",
    "        api = c_authenticate(c_auths, app_num) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='faster'> </a>\n",
    "## Faster version\n",
    "[Back on top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since evaluation may be on code efficiency as well, we created this function just to show \n",
    "# we are able to create faster functions, even though we won't usually need to use them.\n",
    "# To be faster, here we will assume that we start from 0 ids for the twitter pages we\n",
    "# give as an argument, just need a careful usage in order not to waste time downloading \n",
    "# what we already have\n",
    "def main_get_ids_fast(twitter_pages,\n",
    "                      c_auths):  \n",
    "    '''\n",
    "    Purpose: manage apps, pages and iterations before calling\n",
    "             the function to get follower ids of a page,\n",
    "             for each page in twitter_pages\n",
    "    '''\n",
    "    # start from the first app\n",
    "    app_num = 0\n",
    "    pag_rem = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "# each call of this loop allows to download ids of all followers of a twitter page\n",
    "    for twitter_page in twitter_pages:   \n",
    "        \n",
    "        # initialize the tweepy cursor to start from scratch\n",
    "        cursor_val = -1\n",
    "        # login to an api just to get tot num of follw of the twitter page\n",
    "        # an alternative would be to just keep downloading until we\n",
    "        # download less than 70k new ids\n",
    "        api = c_authenticate(c_auths, app_num = app_num)\n",
    "        follw_count = api.get_user(twitter_page).followers_count        \n",
    "# each app downloads 75k follw, calculate how many times we need to iterate among them\n",
    "        num_calls =int (np.ceil ( follw_count/75000))\n",
    "\n",
    "# Skipped at first iteration.     \n",
    "# If we have still a decent amount of pages to use after we're done downloading ids from a page,\n",
    "# stay with the app we just used and take advantage of the other pages we can use.\n",
    "# Expecially useful if we have twitter pages with low values of follw_count,\n",
    "# as, without this step, it would mean to waste a lot of app calls\n",
    "# just to download a couple of pages of results. Otherwise, one could remove this 'if' to\n",
    "# make things faster\n",
    "        if pag_rem > 2:\n",
    "            app_num -=1 \n",
    "            num_pages_to_scroll = pag_rem\n",
    "            #print(\"Switched back to app num {}, still have {} pages\".format(app_num, pag_rem))\n",
    "        else: \n",
    "            num_pages_to_scroll = 15\n",
    "        print(\"calling get_ids with app\", app_num)\n",
    "        # download all followers of this page, going through the num_call we calculated    \n",
    "        app_num, pag_rem = call_get_ids(cursor_val, num_pages_to_scroll, start_time, app_num, num_calls, c_auths, follw_count, twitter_page)\n",
    "        print(\"\\nDownloaded this page followers. Now app_num is\", app_num)\n",
    "        \n",
    "# to further optimize, one could store results only halfway through and on last iter,\n",
    "# instead of storing each time we downloaded all the followers of a page.\n",
    "# The following works, but we should also return g in call_get_ids, and append it to\n",
    "# list_of_lists, on top of removing the storing part in call get_ids. Thus this is\n",
    "# just a pseudo-code.\n",
    "# Dividing by 4 also works\n",
    "        # i += 1\n",
    "        # if int( i% (len(twitter_pages)/2)) == 0 and i != 0:\n",
    "        #    cur.executemany(\"INSERT INTO twitter_data VALUES (?,?)\", (list_of_lists)) \n",
    "        #    con.commit()  \n",
    "        #    list_of_lists= []\n",
    "            \n",
    "    return\n",
    " \n",
    "        \n",
    "def call_get_ids(cursor_val, num_pages_to_scroll, start_time, app_num, num_calls, c_auths,follw_count, twitter_page ):\n",
    "    '''\n",
    "    Purpose: get all follower ids of a twitter_page, managing the\n",
    "             rate limits and storing results at the end\n",
    "    '''\n",
    "# we may be using an app which we already used. In this case, if we will use\n",
    "# only this app, aka one call is enough, we will want to return, as number of\n",
    "# pages remaining for the last app used, not 15 - pages_we_will_use, but, instead,\n",
    "# pages_we_had_left - pages_we_will_use, in order not to overestimate our app capability\n",
    "    if num_calls == 1:\n",
    "        max_pages = num_pages_to_scroll\n",
    "    else:\n",
    "        max_pages = 15\n",
    "        \n",
    "    page_list = []\n",
    "    for call in range(app_num, num_calls):\n",
    "        print(\"i'm at call {}, with app {}\".format(call,app_num))\n",
    "        # authenticate to app number app_num\n",
    "        api = c_authenticate(c_auths, app_num = app_num)\n",
    "        \n",
    "# Calculate if we need less than 15 -or whatever we have left- pages for last iteration;\n",
    "# as above, one may delete this to speed_up if only downloading big datasets.\n",
    "        if call ==  num_calls - 1:\n",
    "            # we already have 75000*num_calls -2 ids\n",
    "            num_pages_to_scroll = min(max_pages, int(np.ceil( (follw_count - (75000*call))/5000 )))\n",
    "            pages_used_last_iter = num_pages_to_scroll\n",
    "            \n",
    "        #print(\"going for num pages\", num_pages_to_scroll)\n",
    "        # download 5k * num_pages_to_scroll ids\n",
    "        tweepy_cursor = tweepy.Cursor(api.followers_ids, id = twitter_page, cursor = cursor_val, count = 5000).pages(num_pages_to_scroll) \n",
    "        \n",
    "        # put them into a list\n",
    "        for tweepy_page in tweepy_cursor:\n",
    "            page_list.extend(tweepy_page) \n",
    "        # update the cursor value\n",
    "        cursor_val = tweepy_cursor.next_cursor\n",
    "        \n",
    "        # change app_num\n",
    "        app_num, start_time, num_pages_to_scroll = change_app(app_num, start_time, c_auths)\n",
    "        \n",
    "        \n",
    "    # store in the db the followers of this twitter page\n",
    "    g = list(zip([twitter_page]*len(page_list), page_list)) \n",
    "    print(\"storing this number of users:\", len(g))\n",
    "    cur.executemany(\"INSERT INTO twitter_data VALUES (?,?)\", (g)) \n",
    "    con.commit()  \n",
    "                                  \n",
    "    return app_num, max_pages - pages_used_last_iter #g\n",
    "\n",
    "\n",
    "def change_app(app_num, start_time, c_auths):\n",
    "    '''\n",
    "    Purpose: update app_num and check if we need to reset it or to sleep\n",
    "    ''' \n",
    "    app_num +=1\n",
    "    # since we are switching app, we can reset num_pages_to_scroll\n",
    "    num_pages_to_scroll = 15\n",
    "    if app_num == len(c_auths):   \n",
    "        # with enough apps it can sleep for 0 secs\n",
    "        sleep_time = max(0, (60*15 + 4) - (time.time() - start_time))\n",
    "        print(\"sleeping for: \", sleep_time)\n",
    "        time.sleep(sleep_time)\n",
    "        start_time = time.time()\n",
    "        app_num = 0\n",
    "    \n",
    "    print(\"switched to app\", app_num)\n",
    "    return app_num, start_time, num_pages_to_scroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling get_ids with app 0\n",
      "i'm at call 0, with app 0\n",
      "switched to app 1\n",
      "storing this number of users: 2901\n",
      "\n",
      "Downloaded this page followers. Now app_num is 1\n",
      "calling get_ids with app 0\n",
      "i'm at call 0, with app 0\n",
      "switched to app 1\n",
      "storing this number of users: 16531\n",
      "\n",
      "Downloaded this page followers. Now app_num is 1\n"
     ]
    }
   ],
   "source": [
    "test = main_get_ids_fast( ['selutbooks', 'bncalcio'], c_auths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='get_pages'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get pages\n",
    "\n",
    "Note: \n",
    "- we store only pages above a certain threshold of followers, to avoid retrieving what we deem un-interesting information, like an user's actual friends\n",
    "- to get the pages liked by the followers of a Twitter page, we need to have at least their ids in the table Users, the best way to achieve this is by calling get_followers on such page, otherwise the faster way is to call get_followers_ids by accordingly changing the storing part (by default it stores in the twitter_data table)\n",
    "\n",
    "\n",
    "[Back on top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_to_use = 'Brand_Followers(backup).db'\n",
    "\n",
    "# connect to the db\n",
    "con, cur =     connect_to_sql(database_name = database_to_use, \n",
    "                              return_df = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([955908060215676938, 975789565016453120,         2317364652,\n",
       "               2462928132,          195368541,         1849299108,\n",
       "                947908681], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to change only twitter_page of interest\n",
    "twitter_page = 'lavazzagroup'\n",
    "\n",
    "# if you use twitter_data table, need to slightly change sql code \n",
    "table = 'Users'                  # or twitter_data\n",
    "column_w_pages = 'twitter_page'  # or brand\n",
    "id_page_cols = 'user_id, pages_liked'         # or followerid\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT {} FROM '{}' WHERE twitter_page == '{}'\".format(id_page_cols, table, twitter_page), con)\n",
    "\n",
    "# create a list of users for which we still have\n",
    "# to retrieve pages liked\n",
    "user_ids = df[df['pages_liked'] == ''][\"user_id\"].to_numpy()\n",
    "#user_ids = df[df[id_page_cols.split()[1]].isnull()][id_page_cols.split()[0][:-1]].to_numpy()\n",
    "\n",
    "print(len(np.unique(user_ids))) # how many pages_liked we still have to retrieve\n",
    "user_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "some users may have gotten blocked by Twitter or may have deleted their account. \n",
    "In this case, we can't even retrieve their number of friends;\n",
    "other users (a lot of them) have private accounts, so we can see how many friends they\n",
    "have, but we can't download them.\n",
    "The best way we found to deal with these is to use a try-except in the first case, and\n",
    "a user_method to check for the second case. Using try-except also in the second case\n",
    "leads to a lot of complications. Indeed, for example, calling api.friends inside a try\n",
    "will still count as a call, so, after 15 such calls, one should switch app, having \n",
    "wasted one app without downloading anything. User methods, instead, have much higher\n",
    "rate limits\n",
    "'''\n",
    "\n",
    "def get_pages(_id, api, threshold = 2500, ret_cur = False, cursor = -1, num_pages = 1):\n",
    "    '''\n",
    "    WHAT DOES IT DO?\n",
    "    Given a single id, it returns a list of the pages followed by that user, given that\n",
    "    they have at least 2500 followers.\n",
    "\n",
    "    CAUTION:\n",
    "    - It does not store them into a db! To do that, create a list [get_pages(_id), _id] for\n",
    "      a number of ids, and feed it to store_pages(multiple_users = list )\n",
    "    - Rate limit after 15 calls, no checks inside the function, since it's called\n",
    "      on a single id, so be careful\n",
    "    '''\n",
    "    #print(\"received id:\", _id)\n",
    "    #try:\n",
    "    page_list = []\n",
    "    # generally, users can't have more than 5k pages followed. We can only retrieve up to 200 per call\n",
    "    tweepy_cursor = tweepy.Cursor(api.friends, count = 200, id= _id, cursor = cursor).pages(num_pages) \n",
    "    for objects in tweepy_cursor:\n",
    "        for page in objects:\n",
    "            # threshold to retrieve only relevant pages followed\n",
    "            if page.followers_count > threshold: \n",
    "                page_list.extend([page.screen_name])\n",
    "    #except:\n",
    "    #    print(\"Private profile, no pages retrieved for this user, returning an empty list\")\n",
    "    #    page_list = []\n",
    "\n",
    "    if ret_cur == False:\n",
    "        return page_list\n",
    "    \n",
    "    else:\n",
    "        return page_list, tweepy_cursor.next_cursor\n",
    "    \n",
    "    \n",
    "def store_pages(page_list = [], _id = 2593342898, \n",
    "                table_name = 'Users', col_w_pages = 'pages_liked', \n",
    "                *multiple_users): # give multiple users as [(_id, page_list),(_id, page_list)...]\n",
    "    '''\n",
    "    WHAT DOES IT DO?\n",
    "    Stores into the db the pages liked by an user, or by more users if multiple_users is used.\n",
    "\n",
    "    HOW ARE THE PAGES STORED?\n",
    "    Since we can store only a single value in the db, either we create a row for each page \n",
    "    followed, for each user, or we compress this info, separating each element with an _***_. \n",
    "    We have chosen the latter option\n",
    "    '''\n",
    "    # store the pages into a column of the db, after joining them into\n",
    "    # a list and separating them with a token\n",
    "\n",
    "    if not multiple_users:\n",
    "        string_pag_seguite = \"\"\n",
    "        for pagina in page_list: \n",
    "            string_pag_seguite +=  pagina + \" _***_ \"\n",
    "\n",
    "        cur.execute(\"UPDATE \"+table_name+\" SET \" + col_w_pages + \" = ('{}') WHERE user_id = \".format(string_pag_seguite[:-7]) + str(_id) )\n",
    "        con.commit()\n",
    "\n",
    "    else:\n",
    "        bag = []\n",
    "        for user in multiple_users[0]: # [0] because it's a karg, so it envelopes all in a tuple\n",
    "            _id =  user[0]\n",
    "            page_list = user[1]\n",
    "            string_pag_seguite = \"\"\n",
    "\n",
    "            for pagina in page_list:\n",
    "                #print(\"id = \", _id)\n",
    "                #print(\"pagina = \", pagina)\n",
    "                string_pag_seguite +=  pagina + \" _***_ \"\n",
    "            bag.append((string_pag_seguite[:-7], _id))\n",
    "        \n",
    "        print(\"table = \", table_name, \"storing into col:\", col_w_pages)\n",
    "        #print(\"Storing into sql the following list\", bag)\n",
    "        cur.executemany(\"UPDATE \"+table_name+\" SET \" + col_w_pages + \" = ? WHERE user_id = ?\",  bag)\n",
    "        con.commit()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def app_num_check(c_auths, app_num, start_time):\n",
    "    '''\n",
    "    Purpose: if we switched through all the apps, sleep and reset app_num index\n",
    "    '''\n",
    "    # nap check, app_num reset\n",
    "    if app_num == len(c_auths):    \n",
    "        sleep_time = max(0, (60*15 + 4) - (time.time() - start_time))\n",
    "        print(\"\\nSleeping for \", sleep_time,\"\\n\")\n",
    "        time.sleep(sleep_time)\n",
    "        start_time = (time.time())\n",
    "        app_num = 0\n",
    "        \n",
    "    # else don't change app_num and don't sleep                  \n",
    "    return app_num, start_time\n",
    "\n",
    "\n",
    "def change_and_download(c_auths, app_num, rem_pages, user_id, cursor, pages, start_time):\n",
    "    '''\n",
    "    Purpose: It is called when we need to change app.\n",
    "             Authentichate to the app in position app_num and download the necessary pages,\n",
    "             up to 15. \n",
    "             If more than 15 pages are needed, it recursively calls itself, upon\n",
    "             updating app_num and calling app_num_check, downloading all pages of that\n",
    "             user (below we limit ourselves to retrieving\n",
    "             a maximum of 4k pages per user, to avoid wasting time downloading even 20k pages\n",
    "             for a single user, they wouldn't be representative of an actual interest)\n",
    "    '''\n",
    "    api = c_authenticate(c_auths, app_num)\n",
    "    print(\"switching to app number:\",app_num)\n",
    "\n",
    "    rem_pages = abs(rem_pages)\n",
    "    if rem_pages > 15:\n",
    "\n",
    "        print(\"changing one more app for this user\")\n",
    "        # download the maximum with the new app, then change app and download the rest\n",
    "        pages_app_switch, cursor = get_pages(_id = user_id, api = api, num_pages = 15, cursor = cursor,  ret_cur = True)\n",
    "        pages.extend(pages_app_switch)\n",
    "        # update the remaining num of pages to download\n",
    "        rem_pages -= 15\n",
    "        # change app\n",
    "        app_num += 1\n",
    "        # check if we reached the last app, needing an index reset and a nap\n",
    "        app_num, start_time = app_num_check(c_auths, app_num, start_time)\n",
    "        # re-call the function to retrieve the rest, using the next app\n",
    "        page_limit, app_num, api, start_time = change_and_download(c_auths, app_num, rem_pages, user_id, cursor, pages, start_time)\n",
    "\n",
    "        return page_limit, app_num, api, start_time\n",
    "\n",
    "\n",
    "    # download the remaining pages for this user\n",
    " \n",
    "    pages_app_switch = get_pages(_id = user_id, api = api, num_pages = rem_pages, cursor = cursor)\n",
    "    pages.extend(pages_app_switch)\n",
    "    print(\"downloaded all liked pages above a threshold of followers ({}) for this user ({})\".format(len(pages), user_id))\n",
    "    \n",
    "    # count how many pages we can download with this app before rate limit\n",
    "    page_limit = 15 - rem_pages\n",
    "\n",
    "    return page_limit, app_num, api, start_time\n",
    "\n",
    "# here we use the above defined functions in a loop with conditions.\n",
    "\n",
    "# Estimate an upward biased number for how many iterations we will need\n",
    "num_iterations = int(np.ceil( len(user_ids) / (15 * len(c_auths)) ))\n",
    "app_num = 0\n",
    "page_limit = 15\n",
    "\n",
    "# initializing app and time count\n",
    "api = c_authenticate(c_auths, 0)\n",
    "start_time = time.time()\n",
    "\n",
    "for user_id in user_ids:\n",
    "    \n",
    "# calculate number of pages to download, check if the account\n",
    "# is blocked using a try-except\n",
    "    try:\n",
    "        print(\"page limit (using app {}) =\".format(app_num), page_limit)\n",
    "\n",
    "        # min to avoid wasting much time on users with absurd amounts of followed pages\n",
    "        num_friends = min (4000, api.get_user(id= user_id).friends_count)\n",
    "        # number of tweepy pages to download\n",
    "        num_pages = int ( np.ceil( num_friends / 200 ))\n",
    "        if num_pages == 0:\n",
    "            print(\"This user has a temporarily limited account, skipping it\")\n",
    "            continue\n",
    "        # rem_pages is the num of tweepy pages we will have left on this app\n",
    "        rem_pages = page_limit - num_pages\n",
    "        #print(\"remaining pages:\", rem_pages)\n",
    "\n",
    "    except:\n",
    "        # go to the next user\n",
    "        print(\"Skipping this user (blocked or deleted profile)\")\n",
    "        # this way we won't check for him again, we will consider he only\n",
    "        # follows the page we know he follows\n",
    "        store_pages([twitter_page], user_id, table, id_page_cols.split()[1])\n",
    "        continue\n",
    "        \n",
    "# if he has private profile, skip him\n",
    "    private_profile = api.get_user(user_id).protected\n",
    "    if private_profile == True:\n",
    "        # this way we won't check for him again\n",
    "        store_pages([twitter_page], user_id, table, id_page_cols.split()[1])\n",
    "        print(\"Skipping this user: he has a private profile. His id: {}\".format(user_id))\n",
    "        continue\n",
    " \n",
    "\n",
    "# we still consider a final broad try-except statement, as some rare\n",
    "# situations can pass through the above screening, for example some\n",
    "# users no longer have an account, but we can still retrieve\n",
    "# how many friends they have and whether their account was private.\n",
    "# This happened, for example, with user_id = 105179723\n",
    "    try:\n",
    "\n",
    "# Now we've done all the checks, and we know how many pages he follows.\n",
    "# We only need to manage the rate limits\n",
    "\n",
    "\n",
    "    # if we can, download and store the pages\n",
    "        if rem_pages >= 0:\n",
    "            print(\"downloading all the liked pages above a threshold of followers ({}) for this user ({})\".format(num_friends, user_id))\n",
    "            # update the num of tweepy pages we can still downl with this app\n",
    "            page_limit = rem_pages\n",
    "            pages = get_pages(_id = user_id, api = api, num_pages = num_pages)\n",
    "            # if one uses a really big number of apps, the storing part can be put\n",
    "            # right before sleeping, calling the below function with its keyword arg\n",
    "            store_pages(pages, user_id, table, id_page_cols.split()[1]) \n",
    "            #print(\"done, next user\")\n",
    "\n",
    "\n",
    "\n",
    "    # if we would reach the rate limit: download what we can,\n",
    "    # switch app, check that we switched apps no more than len(c_auths)\n",
    "    # without resetting and sleeping, download the rest of the pages\n",
    "        else:\n",
    "            print(\"\\ni would reach the rate limit if i downloaded all pages ({}) for this user ({})\".format(num_friends, user_id))\n",
    "\n",
    "            # download what you can with the current app\n",
    "            if page_limit > 0:\n",
    "                print(\"downloading what i can with the current app\")\n",
    "                pages, cursor = get_pages(_id = user_id, api = api, num_pages = page_limit, ret_cur = True)\n",
    "            else: \n",
    "                pages = []\n",
    "                cursor = -1\n",
    "\n",
    "            # update the app number\n",
    "            app_num += 1\n",
    "\n",
    "# check if we passed through all apps and we need to sleep and reset the index\n",
    "            app_num, start_time = app_num_check(c_auths, app_num, start_time)\n",
    "\n",
    "# login to the new app, download the remaining pages and get the num of pages we can still downl\n",
    "            page_limit, app_num, api, start_time = change_and_download(c_auths, app_num, rem_pages, user_id, cursor, pages, start_time)\n",
    "\n",
    "# store pages in the db \n",
    "            store_pages(pages, user_id, table, id_page_cols.split()[1]) \n",
    "\n",
    "            # check that it stored the results (number should go down)\n",
    "            #df = pd.read_sql_query(\"SELECT {} FROM '{}' WHERE twitter_page == '{}'\".format(id_page_cols, table, twitter_page), con)\n",
    "            #print(len(df[df[id_page_cols.split()[1]].isnull()][id_page_cols.split()[0][:-1]].to_numpy()))\n",
    "   \n",
    "    except:\n",
    "        print(\"This user doesn't exist anymore\")\n",
    "        \n",
    "print(\"\\nDone\")\n",
    "\n",
    "# one can use https://tweeterid.com/ to doublecheck that we got an exception because\n",
    "# the profile is indeed private\n",
    "\n",
    "# in the output printed we reached the rate limit\n",
    "# because we restarted the notebook and called the\n",
    "# function again, without waiting 15 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id ='dataset'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Operations\n",
    "\n",
    "[Back on top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape (13991, 10)\n",
      "Final shape (13991, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>twitter_page</th>\n",
       "      <th>user_id</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>status_count</th>\n",
       "      <th>follow_count</th>\n",
       "      <th>friend_count</th>\n",
       "      <th>changed_img</th>\n",
       "      <th>changed_theme</th>\n",
       "      <th>pages_liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>lavazzagroup</td>\n",
       "      <td>38976229</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50cent _***_ Usher _***_ johnlegend _***_ Tige...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>lavazzagroup</td>\n",
       "      <td>1037013378559107074</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nataliehemby _***_ DukeNews _***_ DukeMBB _***...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>lavazzagroup</td>\n",
       "      <td>22661036</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WaffleHouse _***_ Anime _***_ USPSHelp _***_ U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>lavazzagroup</td>\n",
       "      <td>21751873</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VirtualAstro _***_ GaryBarlow _***_ cyprusmail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>lavazzagroup</td>\n",
       "      <td>2468860608</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JGrobicki _***_ TineGoetzsche _***_ swiftlyama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13986</td>\n",
       "      <td>lavazzagroup</td>\n",
       "      <td>28196164</td>\n",
       "      <td>Marketing &amp; Innovation Director @wearesocialit...</td>\n",
       "      <td>Milan, Italy</td>\n",
       "      <td>24929.0</td>\n",
       "      <td>5391.0</td>\n",
       "      <td>744.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BillGates _***_ istsupsan _***_ HowellONeill _...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13987</td>\n",
       "      <td>lavazzagroup</td>\n",
       "      <td>2216552828</td>\n",
       "      <td>I think life is beautiful. As long as you have...</td>\n",
       "      <td></td>\n",
       "      <td>640.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Tokyo2020 _***_ spelacchio _***_ ItalianCommen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13988</td>\n",
       "      <td>lavazzagroup</td>\n",
       "      <td>226117937</td>\n",
       "      <td></td>\n",
       "      <td>Milano</td>\n",
       "      <td>1051.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AirAsiaSupport _***_ stephenfry _***_ rustyroc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13989</td>\n",
       "      <td>lavazzagroup</td>\n",
       "      <td>41348521</td>\n",
       "      <td>Group Account Director @ We Are Social Italia....</td>\n",
       "      <td>Milan - Italy</td>\n",
       "      <td>260.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wearesocial _***_ salvadorcunha _***_ adidasor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13990</td>\n",
       "      <td>lavazzagroup</td>\n",
       "      <td>16190180</td>\n",
       "      <td>Otaku for social+creativity, craft beer, cinem...</td>\n",
       "      <td>Europe • Italy • Milan</td>\n",
       "      <td>24726.0</td>\n",
       "      <td>4699.0</td>\n",
       "      <td>564.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>mikeyk _***_ kevin _***_ yayalexisgay _***_ Ad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13991 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       twitter_page              user_id  \\\n",
       "0      lavazzagroup             38976229   \n",
       "1      lavazzagroup  1037013378559107074   \n",
       "2      lavazzagroup             22661036   \n",
       "3      lavazzagroup             21751873   \n",
       "4      lavazzagroup           2468860608   \n",
       "...             ...                  ...   \n",
       "13986  lavazzagroup             28196164   \n",
       "13987  lavazzagroup           2216552828   \n",
       "13988  lavazzagroup            226117937   \n",
       "13989  lavazzagroup             41348521   \n",
       "13990  lavazzagroup             16190180   \n",
       "\n",
       "                                             description  \\\n",
       "0                                                   None   \n",
       "1                                                   None   \n",
       "2                                                   None   \n",
       "3                                                   None   \n",
       "4                                                   None   \n",
       "...                                                  ...   \n",
       "13986  Marketing & Innovation Director @wearesocialit...   \n",
       "13987  I think life is beautiful. As long as you have...   \n",
       "13988                                                      \n",
       "13989  Group Account Director @ We Are Social Italia....   \n",
       "13990  Otaku for social+creativity, craft beer, cinem...   \n",
       "\n",
       "                     location  status_count  follow_count  friend_count  \\\n",
       "0                        None           NaN           NaN           NaN   \n",
       "1                        None           NaN           NaN           NaN   \n",
       "2                        None           NaN           NaN           NaN   \n",
       "3                        None           NaN           NaN           NaN   \n",
       "4                        None           NaN           NaN           NaN   \n",
       "...                       ...           ...           ...           ...   \n",
       "13986            Milan, Italy       24929.0        5391.0         744.0   \n",
       "13987                                 640.0         233.0         297.0   \n",
       "13988                  Milano        1051.0         468.0         184.0   \n",
       "13989           Milan - Italy         260.0         208.0         157.0   \n",
       "13990  Europe • Italy • Milan       24726.0        4699.0         564.0   \n",
       "\n",
       "       changed_img  changed_theme  \\\n",
       "0              NaN            NaN   \n",
       "1              NaN            NaN   \n",
       "2              NaN            NaN   \n",
       "3              NaN            NaN   \n",
       "4              NaN            NaN   \n",
       "...            ...            ...   \n",
       "13986          0.0            0.0   \n",
       "13987          0.0            0.0   \n",
       "13988          0.0            1.0   \n",
       "13989          0.0            0.0   \n",
       "13990          0.0            0.0   \n",
       "\n",
       "                                             pages_liked  \n",
       "0      50cent _***_ Usher _***_ johnlegend _***_ Tige...  \n",
       "1      nataliehemby _***_ DukeNews _***_ DukeMBB _***...  \n",
       "2      WaffleHouse _***_ Anime _***_ USPSHelp _***_ U...  \n",
       "3      VirtualAstro _***_ GaryBarlow _***_ cyprusmail...  \n",
       "4      JGrobicki _***_ TineGoetzsche _***_ swiftlyama...  \n",
       "...                                                  ...  \n",
       "13986  BillGates _***_ istsupsan _***_ HowellONeill _...  \n",
       "13987  Tokyo2020 _***_ spelacchio _***_ ItalianCommen...  \n",
       "13988  AirAsiaSupport _***_ stephenfry _***_ rustyroc...  \n",
       "13989  wearesocial _***_ salvadorcunha _***_ adidasor...  \n",
       "13990  mikeyk _***_ kevin _***_ yayalexisgay _***_ Ad...  \n",
       "\n",
       "[13991 rows x 10 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Cleaning '''\n",
    "# Check the followers we downloaded \n",
    "table = 'Users'    # Users contains detailed info, twitter_data only contains ids by page followed\n",
    "column_w_pages = 'twitter_page' #  twitter_page --- brand\n",
    "column_w_ids = 'user_id'        #  user_id --- followerid\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM '{}'\" .format(table), con) \n",
    "# remove duplicates in the whole dataset\n",
    "df = rem_duplicates_dataset(df, column_w_page_name = column_w_pages, column_with_duplicates= column_w_ids)\n",
    "\n",
    "# update the table, letting in it only non-duplicated values\n",
    "#df.drop(columns = \"index\", inplace = True)\n",
    "#df.to_sql(name = table, if_exists = 'replace', con = con, index = False)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13991"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check: lavazza should have around 14k followers\n",
    "page_of_interest = 'lavazzagroup'\n",
    "len(df[df[column_w_pages] == page_of_interest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MonsterEnergy', 'MonsterGaming', 'Nespresso', 'Starbucks',\n",
       "       'lavazzagroup', 'Bulgariofficial', 'letortedirenato',\n",
       "       'CakeDesignIta', 'ricettedolci1', 'DolciRicette',\n",
       "       'Dulcisssinforno', 'DolciSemplici', 'LeTortine', 'OXO',\n",
       "       'HydroFlask', 'swellbottle', 'YETICoolers', 'MonsterEnergyIT',\n",
       "       'redbullITA', 'burn_ITA', 'ROLEX', 'TiffanyAndCo', 'Ferrari',\n",
       "       'ChiaraFerragni', 'camihawke', 'marianodivaio', 'sferaebbasta',\n",
       "       'saluis97', 'aperolspritzita', 'CampariItalia', 'BACARDI_IT',\n",
       "       'HandpressoNews', 'MaisonValentino', 'Prada', 'Fendi',\n",
       "       'PaneraiOfficial', 'Azimut_Yachts', 'armani', 'RivaYacht',\n",
       "       'FerrettiGroup', 'GoPro', 'ray_ban', 'hamiltonwatch',\n",
       "       'GlycineWatch', 'TISSOT', 'GentryWatch', 'LandRover',\n",
       "       'LandRoverItalia', 'style_italia', 'VanityFairIt', 'esquireitalia',\n",
       "       'esquire', 'BoggiMilano', 'theonlyparmesan', 'ideericette',\n",
       "       'illyIT', 'CaffeVergnano', 'Kimbo_It', 'LavazzaUK', 'Lamborghini',\n",
       "       'selutbooks', 'bncalcio', 'canc', 'cance'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' see the pages from which we collected data '''\n",
    "df[column_w_pages].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
